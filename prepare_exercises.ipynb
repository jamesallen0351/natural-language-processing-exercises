{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f117d0cc",
   "metadata": {},
   "source": [
    "# Prepare Exercises\n",
    "\n",
    "The end result of this exercise should be a file named prepare.py that defines the requested functions.\n",
    "\n",
    "In this exercise we will be defining some functions to prepare textual data. These functions should apply equally well to both the codeup blog articles and the news articles that were previously acquired.\n",
    "\n",
    "### 1. Define a function named basic_clean. It should take in a string and apply some basic text cleaning to it:\n",
    "\n",
    "- Lowercase everything\n",
    "- Normalize unicode characters\n",
    "- Replace anything that is not a letter, number, whitespace or a single quote.\n",
    "\n",
    "### 2. Define a function named tokenize. It should take in a string and tokenize all the words in the string.\n",
    "\n",
    "### 3. Define a function named stem. It should accept some text and return the text after applying stemming to all the words.\n",
    "\n",
    "### 4. Define a function named lemmatize. It should accept some text and return the text after applying lemmatization to each word.\n",
    "\n",
    "### 5. Define a function named remove_stopwords. It should accept some text and return the text after removing all the stopwords.This function should define two optional parameters, extra_words and exclude_words. These parameters should define any additional stop words to include, and any words that we don't want to remove.\n",
    "\n",
    "### 6. Use your data from the acquire to produce a dataframe of the news articles. Name the dataframe news_df.\n",
    "\n",
    "### 7. Make another dataframe for the Codeup blog posts. Name the dataframe codeup_df.\n",
    "\n",
    "### 8. For each dataframe, produce the following columns:\n",
    "\n",
    "- title to hold the title\n",
    "- original to hold the original article/post content\n",
    "- clean to hold the normalized and tokenized original with the stopwords removed.\n",
    "- stemmed to hold the stemmed version of the cleaned data.\n",
    "- lemmatized to hold the lemmatized version of the cleaned data.\n",
    "\n",
    "### 9. Ask yourself:\n",
    "\n",
    "- If your corpus is 493KB, would you prefer to use stemmed or lemmatized text?\n",
    "- If your corpus is 25MB, would you prefer to use stemmed or lemmatized text?\n",
    "- If your corpus is 200TB of text and you're charged by the megabyte for your hosted computational resources, would you prefer to use stemmed or lemmatized text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7355dd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import unicodedata\n",
    "import re\n",
    "import json\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filter=\"ignore\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb7a53fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 basic_clean\n",
    "\n",
    "def basic_clean(string):\n",
    "    '''\n",
    "    basic cleaning function: lowercase, normalize, and remove characters\n",
    "    ''' \n",
    "    # lowercase everthing\n",
    "    string = string.lower()\n",
    "    # normalize unicode charaters\n",
    "    string = unicodedata.normalize('NFKD', string)\\\n",
    "    .encode('ascii','ignore')\\\n",
    "    .decode('utf-8', 'ignore')\n",
    "    # replace anything that is not a letter, number, whitespace or a single quote\n",
    "    string = re.sub(r\"[^a-z0-9'\\s]\", '', string)\n",
    "    string = re.sub(r\"\\n\", '', string)\n",
    "    \n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bb545f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a line of text\n",
    "text = \"Our 25th Anniversary Ale is here! Learn about this year's blend and find out where it's available near you here: https://fal.cn/3jkQt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a2faa41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"our 25th anniversary ale is here learn about this year's blend and find out where it's available near you here httpsfalcn3jkqt\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing out my basic_clean function with the text\n",
    "basic_clean(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ff818f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize function\n",
    "\n",
    "def tokenize(string):\n",
    "    '''\n",
    "    function to take a string and tokenize all the words\n",
    "    '''\n",
    "    # make the tokenizer\n",
    "    tokenizer = nltk.tokenize.ToktokTokenizer()\n",
    "    # use the tokenizer\n",
    "    string = tokenizer.tokenize(string, return_str=True)\n",
    "    \n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "503f62d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing out the tokenize function on the text\n",
    "tokenized = tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8073b26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stem function\n",
    "\n",
    "def stem(string):\n",
    "    '''\n",
    "    function to take some text and apply stemming to all the words\n",
    "    '''\n",
    "    # Create porter stemmer.\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    \n",
    "    # Use the stemmer to stem each word in the list of words we created by using split.\n",
    "    stems = [ps.stem(word) for word in string.split()]\n",
    "    \n",
    "    # Join our lists of words into a string again and assign to a variable.\n",
    "    string = ' '.join(stems)\n",
    "    \n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "add20ea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"our 25th anniversari ale is here! learn about thi year' blend and find out where it' avail near you here: https://fal.cn/3jkqt\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trying out the stem function on the text\n",
    "stem(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8e3c9fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatize function\n",
    "\n",
    "def lemmatize(string):\n",
    "    '''\n",
    "    function to take some text and apply lemmatization to each word\n",
    "    '''\n",
    "    # Create the lemmatizer.\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    \n",
    "    # Use the lemmatizer on each word in the list of words we created by using split.\n",
    "    lemmas = [wnl.lemmatize(word) for word in string.split()]\n",
    "    \n",
    "    # Join our list of words into a string again and assign to a variable.\n",
    "    string = ' '.join(lemmas)\n",
    "    \n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a37d61ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Our 25th Anniversary Ale is here! Learn about this year's blend and find out where it's available near you here: https://fal.cn/3jkQt\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing out the lemmatize function on the text\n",
    "lemmatize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "145ca990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove_stopwords function\n",
    "\n",
    "def remove_stopwords(string, extra_words=[], exclude_words=[]):\n",
    "    '''\n",
    "    function to take some text and remove all the stopwords\n",
    "    '''\n",
    "    # Create stopword_list.\n",
    "    stopword_list = stopwords.words('english')\n",
    "    \n",
    "    # Remove 'exclude_words' from stopword_list to keep these in my text.\n",
    "    stopword_list = set(stopword_list) - set(exclude_words)\n",
    "    \n",
    "    # Add in 'extra_words' to stopword_list.\n",
    "    stopword_list = stopword_list.union(set(extra_words))\n",
    "\n",
    "    # Split words in string.\n",
    "    words = string.split()\n",
    "    \n",
    "    # Create a list of words from my string with stopwords removed and assign to variable.\n",
    "    filtered_words = [word for word in words if word not in stopword_list]\n",
    "    \n",
    "    # Join words in the list back into strings and assign to a variable.\n",
    "    string_without_stopwords = ' '.join(filtered_words)\n",
    "    \n",
    "    return string_without_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "25b336f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Our 25th Anniversary Ale here! Learn year's blend find available near here: https://fal.cn/3jkQt\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing out remove_stopwords function on the text\n",
    "remove_stopwords(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b1b1db19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import acquire\n",
    "from time import strftime\n",
    "import warnings\n",
    "warnings.filter=\"ignore\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7bae9a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use your data from the acquire to produce a dataframe of the news articles. Name the dataframe news_df\n",
    "\n",
    "# define categories\n",
    "categories = [\"business\", \"sports\", \"technology\", \"entertainment\"]\n",
    "\n",
    "# use get_all_new_article function from acquire.py file \n",
    "\n",
    "news_df = acquire.get_all_news_articles(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9820b95e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Facebook changes its company name to 'Meta'</td>\n",
       "      <td>Facebook on Thursday announced it's changing t...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'Man who takes 6 months parental leave is a lo...</td>\n",
       "      <td>Several Twitter users criticised US-based Pala...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Delhi HC notice to RBI, SBI over banning UPI p...</td>\n",
       "      <td>The Delhi High Court on Thursday issued notice...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Who are the top 10 new entrants on Hurun India...</td>\n",
       "      <td>Ace investor Rakesh Jhunjhunwala is the top ne...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Legacy companies eat Ola, Ather, Tork &amp; SmartE...</td>\n",
       "      <td>Bajaj Auto's MD Rajiv Bajaj on Thursday took a...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Looking at February 2022: Ali Fazal on wedding...</td>\n",
       "      <td>Actor Ali Fazal has revealed that his wedding ...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Chose to do stunts myself, no body doubles: Ni...</td>\n",
       "      <td>Actress Nitu Chandra has revealed that she cho...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Somewhere in my mind I knew I'll survive: Mahe...</td>\n",
       "      <td>Mahesh Manjrekar, who was diagnosed with urina...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Witnessed exorcism as child, most frightening ...</td>\n",
       "      <td>Actor Emraan Hashmi has said that he witnessed...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Pinch myself every day: Chris Evans on voicing...</td>\n",
       "      <td>Actor Chris Evans has said that he pinches him...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  \\\n",
       "0         Facebook changes its company name to 'Meta'   \n",
       "1   'Man who takes 6 months parental leave is a lo...   \n",
       "2   Delhi HC notice to RBI, SBI over banning UPI p...   \n",
       "3   Who are the top 10 new entrants on Hurun India...   \n",
       "4   Legacy companies eat Ola, Ather, Tork & SmartE...   \n",
       "..                                                ...   \n",
       "95  Looking at February 2022: Ali Fazal on wedding...   \n",
       "96  Chose to do stunts myself, no body doubles: Ni...   \n",
       "97  Somewhere in my mind I knew I'll survive: Mahe...   \n",
       "98  Witnessed exorcism as child, most frightening ...   \n",
       "99  Pinch myself every day: Chris Evans on voicing...   \n",
       "\n",
       "                                              content       category  \n",
       "0   Facebook on Thursday announced it's changing t...       business  \n",
       "1   Several Twitter users criticised US-based Pala...       business  \n",
       "2   The Delhi High Court on Thursday issued notice...       business  \n",
       "3   Ace investor Rakesh Jhunjhunwala is the top ne...       business  \n",
       "4   Bajaj Auto's MD Rajiv Bajaj on Thursday took a...       business  \n",
       "..                                                ...            ...  \n",
       "95  Actor Ali Fazal has revealed that his wedding ...  entertainment  \n",
       "96  Actress Nitu Chandra has revealed that she cho...  entertainment  \n",
       "97  Mahesh Manjrekar, who was diagnosed with urina...  entertainment  \n",
       "98  Actor Emraan Hashmi has said that he witnessed...  entertainment  \n",
       "99  Actor Chris Evans has said that he pinches him...  entertainment  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# looking at the news_df \n",
    "news_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1510d324",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     facebook thursday announced ' changing company...\n",
       "1     several twitter user criticised usbased palant...\n",
       "2     delhi high court thursday issued notice rbi sb...\n",
       "3     ace investor rakesh jhunjhunwala top new entra...\n",
       "4     bajaj auto ' md rajiv bajaj thursday took jibe...\n",
       "                            ...                        \n",
       "95    actor ali fazal ha revealed wedding plan actre...\n",
       "96    actress nitu chandra ha revealed chose stunt w...\n",
       "97    mahesh manjrekar wa diagnosed urinary bladder ...\n",
       "98    actor emraan hashmi ha said witnessed exorcism...\n",
       "99    actor chris evans ha said pinch every day voic...\n",
       "Name: content, Length: 100, dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use all the functions to see if they work on news_df's content column\n",
    "news_df['content'].apply(basic_clean)\\\n",
    ".apply(tokenize)\\\n",
    ".apply(lemmatize)\\\n",
    ".apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "33d676d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jamesallen/codeup-data-science/natural-language-processing-exercises/acquire.py:14: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 14 of the file /Users/jamesallen/codeup-data-science/natural-language-processing-exercises/acquire.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  soup = BeautifulSoup(response.text)\n",
      "/Users/jamesallen/codeup-data-science/natural-language-processing-exercises/acquire.py:21: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 21 of the file /Users/jamesallen/codeup-data-science/natural-language-processing-exercises/acquire.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  soup = BeautifulSoup(response.text)\n"
     ]
    }
   ],
   "source": [
    "# Make another dataframe for the Codeup blog posts. Name the dataframe codeup_df\n",
    "today = strftime('%Y-%m-%d')\n",
    "codeup_df = acquire.get_blog_articles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d53e74f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>published</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Codeup Launches First Podcast: Hire Tech</td>\n",
       "      <td>Aug 25, 2021</td>\n",
       "      <td>Any podcast enthusiasts out there? We are plea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Why Should I Become a System Administrator?</td>\n",
       "      <td>Aug 23, 2021</td>\n",
       "      <td>With so many tech careers in demand, why choos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Announcing our Candidacy for Accreditation!</td>\n",
       "      <td>Jun 30, 2021</td>\n",
       "      <td>Did you know that even though we’re an indepen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Codeup Takes Over More of the Historic Vogue B...</td>\n",
       "      <td>Jun 21, 2021</td>\n",
       "      <td>Codeup is moving into another floor of our His...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Inclusion at Codeup During Pride Month (and Al...</td>\n",
       "      <td>Jun 4, 2021</td>\n",
       "      <td>Happy Pride Month! Pride Month is a dedicated ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title     published  \\\n",
       "0           Codeup Launches First Podcast: Hire Tech  Aug 25, 2021   \n",
       "1        Why Should I Become a System Administrator?  Aug 23, 2021   \n",
       "2        Announcing our Candidacy for Accreditation!  Jun 30, 2021   \n",
       "3  Codeup Takes Over More of the Historic Vogue B...  Jun 21, 2021   \n",
       "4  Inclusion at Codeup During Pride Month (and Al...   Jun 4, 2021   \n",
       "\n",
       "                                             content  \n",
       "0  Any podcast enthusiasts out there? We are plea...  \n",
       "1  With so many tech careers in demand, why choos...  \n",
       "2  Did you know that even though we’re an indepen...  \n",
       "3  Codeup is moving into another floor of our His...  \n",
       "4  Happy Pride Month! Pride Month is a dedicated ...  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codeup_df.head() # check_yo_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "df680447",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  For each dataframe, produce the following columns: title, original, clean, stemmed, lemmatized\n",
    "\n",
    "def prep_article_data(df, column, extra_words=[], exclude_words=[]):\n",
    "    '''\n",
    "    This function take in a df and the string name for a text column with \n",
    "    option to pass lists for extra_words and exclude_words and\n",
    "    returns a df with the text article title, original text, stemmed text,\n",
    "    lemmatized text, cleaned, tokenized, & lemmatized text with stopwords removed.\n",
    "    '''\n",
    "    df['clean'] = df[column].apply(basic_clean)\\\n",
    "                            .apply(tokenize)\\\n",
    "                            .apply(remove_stopwords, \n",
    "                                   extra_words=extra_words, \n",
    "                                   exclude_words=exclude_words)\n",
    "    \n",
    "    df['stemmed'] = df[column].apply(basic_clean)\\\n",
    "                            .apply(tokenize)\\\n",
    "                            .apply(stem)\\\n",
    "                            .apply(remove_stopwords, \n",
    "                                   extra_words=extra_words, \n",
    "                                   exclude_words=exclude_words)\n",
    "    \n",
    "    df['lemmatized'] = df[column].apply(basic_clean)\\\n",
    "                            .apply(tokenize)\\\n",
    "                            .apply(lemmatize)\\\n",
    "                            .apply(remove_stopwords, \n",
    "                                   extra_words=extra_words, \n",
    "                                   exclude_words=exclude_words)\n",
    "    \n",
    "    return df[['title', column,'clean', 'stemmed', 'lemmatized']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3058b803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>clean</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Facebook changes its company name to 'Meta'</td>\n",
       "      <td>Facebook on Thursday announced it's changing t...</td>\n",
       "      <td>facebook thursday announced ' changing company...</td>\n",
       "      <td>facebook thursday announc ' chang compani ' na...</td>\n",
       "      <td>facebook thursday announced ' changing company...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'Man who takes 6 months parental leave is a lo...</td>\n",
       "      <td>Several Twitter users criticised US-based Pala...</td>\n",
       "      <td>several twitter users criticised usbased palan...</td>\n",
       "      <td>sever twitter user criticis usbas palantir tec...</td>\n",
       "      <td>several twitter user criticised usbased palant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Delhi HC notice to RBI, SBI over banning UPI p...</td>\n",
       "      <td>The Delhi High Court on Thursday issued notice...</td>\n",
       "      <td>delhi high court thursday issued notice rbi sb...</td>\n",
       "      <td>delhi high court thursday issu notic rbi sbi n...</td>\n",
       "      <td>delhi high court thursday issued notice rbi sb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Who are the top 10 new entrants on Hurun India...</td>\n",
       "      <td>Ace investor Rakesh Jhunjhunwala is the top ne...</td>\n",
       "      <td>ace investor rakesh jhunjhunwala top new entra...</td>\n",
       "      <td>ace investor rakesh jhunjhunwala top new entra...</td>\n",
       "      <td>ace investor rakesh jhunjhunwala top new entra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Legacy companies eat Ola, Ather, Tork &amp; SmartE...</td>\n",
       "      <td>Bajaj Auto's MD Rajiv Bajaj on Thursday took a...</td>\n",
       "      <td>bajaj auto ' md rajiv bajaj thursday took jibe...</td>\n",
       "      <td>bajaj auto ' md rajiv bajaj thursday took jibe...</td>\n",
       "      <td>bajaj auto ' md rajiv bajaj thursday took jibe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0        Facebook changes its company name to 'Meta'   \n",
       "1  'Man who takes 6 months parental leave is a lo...   \n",
       "2  Delhi HC notice to RBI, SBI over banning UPI p...   \n",
       "3  Who are the top 10 new entrants on Hurun India...   \n",
       "4  Legacy companies eat Ola, Ather, Tork & SmartE...   \n",
       "\n",
       "                                             content  \\\n",
       "0  Facebook on Thursday announced it's changing t...   \n",
       "1  Several Twitter users criticised US-based Pala...   \n",
       "2  The Delhi High Court on Thursday issued notice...   \n",
       "3  Ace investor Rakesh Jhunjhunwala is the top ne...   \n",
       "4  Bajaj Auto's MD Rajiv Bajaj on Thursday took a...   \n",
       "\n",
       "                                               clean  \\\n",
       "0  facebook thursday announced ' changing company...   \n",
       "1  several twitter users criticised usbased palan...   \n",
       "2  delhi high court thursday issued notice rbi sb...   \n",
       "3  ace investor rakesh jhunjhunwala top new entra...   \n",
       "4  bajaj auto ' md rajiv bajaj thursday took jibe...   \n",
       "\n",
       "                                             stemmed  \\\n",
       "0  facebook thursday announc ' chang compani ' na...   \n",
       "1  sever twitter user criticis usbas palantir tec...   \n",
       "2  delhi high court thursday issu notic rbi sbi n...   \n",
       "3  ace investor rakesh jhunjhunwala top new entra...   \n",
       "4  bajaj auto ' md rajiv bajaj thursday took jibe...   \n",
       "\n",
       "                                          lemmatized  \n",
       "0  facebook thursday announced ' changing company...  \n",
       "1  several twitter user criticised usbased palant...  \n",
       "2  delhi high court thursday issued notice rbi sb...  \n",
       "3  ace investor rakesh jhunjhunwala top new entra...  \n",
       "4  bajaj auto ' md rajiv bajaj thursday took jibe...  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing out the prep_article_data function on the news_df\n",
    "prep_article_data(news_df, 'content', extra_words = ['ha'], exclude_words = ['no']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3640fbe6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
